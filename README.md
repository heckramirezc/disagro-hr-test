# Prueba T√©cnica: Flujo ETL y API de (Pageviews de Wikipedia)

## üöÄ Resumen del Proyecto

Este proyecto es la resoluci√≥n de una prueba t√©cnica para el puesto de Desarrollador(a) Full Stack en DISAGRO. El objetivo principal es construir un **flujo ETL (Extract, Transform, Load)** robusto en Python, que procese datos p√∫blicos de BigQuery (Wikipedia Pageviews), almacene los resultados en una base de datos relacional y exponga una **API RESTful** para consultas de negocio.

Adem√°s, he incorporado una **aplicaci√≥n frontend multi-plataforma desarrollada en Flutter**, demostrando la integraci√≥n completa de la soluci√≥n y mi capacidad para construir aplicaciones de extremo a extremo (Full Stack) que cumplen con los requisitos funcionales y de presentaci√≥n.

El desarrollo, seguimiento y gesti√≥n de tareas de este proyecto fueron realizados utilizando [**GitHub Issues y GitHub Projects**](https://github.com/users/heckramirezc/projects/1) dentro de este repositorio, lo que refleja un enfoque organizado y transparente en el proceso de desarrollo.

## üõ†Ô∏è Stack Tecnol√≥gico

| **Componente**    | **Tecnolog√≠a**                | **Prop√≥sito**                                                | Justificaci√≥n                                                |
| ----------------- | ----------------------------- | ------------------------------------------------------------ | ------------------------------------------------------------ |
| **Orquestaci√≥n**  | Docker Compose                | Definici√≥n y gesti√≥n del entorno de tres servicios.          | Requisito expl√≠cito de la prueba.                            |
| **Base de Datos** | PostgreSQL 17 + Docker        | Almacenamiento de datos y soporte para Vistas Materializadas. | PostgreSQL tiene excelente soporte para modelos de datos complejos (como el modelo estrella solicitado). |
| **Capa de Datos** | Python 3.11, Pandas, Psycopg2 | **E**xtracci√≥n (BigQuery), **T**ransformaci√≥n (C√°lculo de m√©tricas), **L**oad (UPSERT a PostgreSQL). | Requisito expl√≠cito de la prueba.                            |
| **API Backend**   | NestJS (Node.js/TypeScript)   | Exposici√≥n de *endpoints* de consulta, paginaci√≥n y documentaci√≥n (Swagger/Redoc). | Aunque la prueba permit√≠a "lenguaje a tu elecci√≥n", opt√© por NestJS para la API debido a mi s√≥lida experiencia en Node.js y TypeScript, y la menci√≥n de NestJS en la entrevista. |
| **Cloud**         | Google BigQuery (Fuente)      | Fuente de datos p√∫blicos de Wikipedia Pageviews.             | Requisito expl√≠cito de la prueba.                            |
| Frontend          | Flutter(PENDIENTE)            | Visualizaci√≥n de datos / Orquestaci√≥n de ETL                 | Aunque no expl√≠citamente requerido en el documento, se mencion√≥ el frontend en la llamada. Aprovechando mi experiencia en Flutter y la versatilidad de la plataforma, constru√≠ un frontend multi-plataforma. La versi√≥n web est√° desplegada en **Firebase Hosting**, mostrando mi familiaridad con el ecosistema de Google. |



## ‚ú® Funcionalidades Implementadas

### **1. Flujo ETL (Python en contenedor Docker - Cloud Run para producci√≥n):**

*   **Extracci√≥n de BigQuery:**
    *   Conexi√≥n parametrizada a `bigquery-public-data.wikipedia.pageviews_2024`.
    *   Extracci√≥n de datos por rango de fechas e idiomas espec√≠ficos.
    *   Filtrado inicial para limitar el volumen de datos y acotar al tr√°fico humano (decisi√≥n documentada en el c√≥digo ETL).
    *   Recuperaci√≥n de campos m√≠nimos: fecha (agregada a d√≠a), wiki/idioma, t√≠tulo, vistas.
*   **Transformaci√≥n:**
    *   Normalizaci√≥n de t√≠tulos a un formato consistente (sin espacios/diacr√≠ticos).
    *   Unificaci√≥n del tr√°fico de escritorio y m√≥vil (`en` y `en.m` se combinan para un `views_total` unificado por idioma).
    *   Exclusi√≥n de t√≠tulos no representativos de contenido (criterios documentados en el ETL).
    *   C√°lculo de m√©tricas derivadas: `views_total` (suma de plataformas), `avg_views_7d`, `avg_views_28d`, `variations` y `trend_score` (basado en z-score).
*   **Carga en PostgreSQL:**
    *   Dise√±o de un modelo estrella (`dim_page`, `fact_pageviews_daily`) para datos anal√≠ticos.
    *   Creaci√≥n de la tabla `etl_jobs` para registrar el estado de cada ejecuci√≥n del ETL desde el frontend.
    *   L√≥gica de inserci√≥n/actualizaci√≥n de datos en las tablas de la base de datos.
    *   Refresco de vistas materializadas post-carga.

### **2. API de Datos (NestJS en contenedor Docker - Cloud Run para producci√≥n):**

*   **Endpoints RESTful:**

    La API proporciona tres *endpoints* para consultar los datos optimizados.

    | **Ruta**                   | **Descripci√≥n**                                              | **Optimizaci√≥n**                                         |
    | -------------------------- | ------------------------------------------------------------ | -------------------------------------------------------- |
    | **GET /api/page/top**      | Retorna el ranking de las p√°ginas m√°s vistas por d√≠a e idioma. Soporta paginaci√≥n (`limit`, `offset`). | Consulta `mv_top_n_daily_by_language`.                   |
    | **GET /api/page/trending** | Retorna las p√°ginas cuyo `trend_score` excede el umbral de `2.0`. Soporta paginaci√≥n. | Consulta `mv_trending_daily`.                            |
    | **GET /api/page/:title**   | Retorna la serie hist√≥rica diaria de vistas y m√©tricas (7d, 28d, trend) para una p√°gina espec√≠fica en un rango de fechas. | Consulta `fact_pageviews_daily` filtrando por `page_id`. |

    

*   **Formato de Respuesta:**

    *   JSON consistente con `items`, `page`, `page_size`, `total`, `params`.

*   **Documentaci√≥n Interactiva:**

    La documentaci√≥n est√° disponible autom√°ticamente despu√©s de iniciar el servicio `api`:

    - **Swagger UI:** `http://localhost:3000/api-docs`
    - **Redoc (Vista Amigable):** `http://localhost:3000/`

    

    La API proporciona endpoints para la orquestaci√≥n desde el frontend del ETL:

    *   `POST /api/etl/start`: Inicia un nuevo trabajo ETL (invoca Cloud Run del ETL as√≠ncronamente).
    *   `GET /api/etl/status/{jobId}`: Permite consultar el estado actual de un trabajo ETL espec√≠fico.

### **3. Pruebas:**

*   El proyecto incluye pruebas unitarias y de integraci√≥n para asegurar la confiabilidad del c√≥digo.

| **√Årea** | **Tecnolog√≠a**     | **Cobertura**                                                |
| -------- | ------------------ | ------------------------------------------------------------ |
| **ETL**  | `pytest`           | Pruebas para las funciones de transformaci√≥n de datos (normalizaci√≥n, m√©tricas). |
| **API**  | `jest` / Supertest | Pruebas unitarias para servicios y pruebas de integraci√≥n para los controladores de la API (`GET /top`,`GET /trending`, `GET /page/:title`) validando estatus HTTP, estructura JSON y l√≥gica de paginaci√≥n. |

---

## üöÄ Configuraci√≥n del Entorno y Ejecuci√≥n

El proyecto est√° dise√±ado para ejecutarse completamente con `docker compose`.

### 1.1. Prerrequisitos

Aseg√∫rate de tener instalados:

- **Docker** y **Docker Compose** (o Docker Desktop).
- **Node.js y Python** (opcional, solo para desarrollo local fuera de Docker).

### 1.2. Configuraci√≥n de Credenciales

Este proyecto requiere acceso a Google BigQuery y credenciales de base de datos.

1. **Credenciales de Google BigQuery:**

   - Crea una cuenta de servicio de Google Cloud con el rol de **Lector de datos de BigQuery** (o permisos equivalentes).
   - Descarga el archivo JSON de la clave de servicio.
   - Copia el contenido del archivo JSON en **`api/bigquery_key.json`**.
   - Este archivo es montado en el contenedor ETL a trav√©s del `docker-compose.yml`.

2. **Variables de Entorno (.env):**

   - En la raiz del proyecto se encuentra el archivo **`.env`**.
   - Se encontar√° la siguiente configuraci√≥n para desarrollo.

   ```
   # --- PostgreSQL Configuration ---
   DB_HOST=postgres
   DB_PORT=5432
   DB_NAME=disagro_db
   DB_USER=disagro_user
   DB_PASSWORD=your_secure_password
   DB_SSL_MODE=disable # Usar 'require' en producci√≥n
   
   # --- API Configuration ---
   PORT=3000
   CORS_ORIGIN=http://localhost:3000,http://localhost:3001 # Origen del frontend
   
   # --- BigQuery Configuration---
   # El ID del proyecto donde se ejecuta la consulta de BigQuery.
   BIGQUERY_PROJECT_ID=disagro-hr-test 
   ```

### 1.3. Instrucciones de Ejecuci√≥n

1. Instalaci√≥n de paquetes (API):

   En terminal ejecuta **`cd api/`** para ir al directorio de la API y proceder a instlar los paquetes. IMPORTANTES: sin estar presentes los paquetes el contenedor de API puede presentar inconvenientes de lanzamiento. 

   ```
   npm install
   ```

2. Construir y Lanzar Contenedores (DB, ETL, API):

   Construye las im√°genes y levanta los servicios.

   ```
   docker-compose up
   ```

3. Los contenedores ETL y API son directamente dependientes del contenedor DB, por lo que no iniciar√°n hasta que el estado de DB sea saludable:

   El servicio postgres ejecutar√° autom√°ticamente el DDL para crear todas las tablas, √≠ndices y vistas materializadas (ubicados en el volumen ./db/ddl).

4. Sobre contenedor ETL:

   El servicio etl est√° configurado como un job que se ejecuta y detiene autom√°ticamente. Aseg√∫rate de que los servicios postgres y api est√©n levantados.

   *Nota: El ETL registrar√° su progreso en la tabla `etl_jobs` de PostgreSQL.*

---

## ‚úÖ Ejecuci√≥n de Pruebas Automatizadas

El proyecto incluye dos conjuntos de pruebas fundamentales:

### 1. Pruebas del API (NestJS / TypeScript)

Estas pruebas incluyen tanto **Pruebas Unitarias** (para la l√≥gica de negocio en los Servicios) como **Pruebas de Integraci√≥n** (para la capa HTTP de los Controladores, verificando rutas, DTOs y validaci√≥n).



| **Comando**                                                 | **Descripci√≥n**                                              |
| ----------------------------------------------------------- | ------------------------------------------------------------ |
| `docker exec -it disagro_hr_api npm run test`               | Ejecuta todas las pruebas unitarias y de integraci√≥n del API. |
| `docker exec -it disagro_hr_api npm run test -- --verbose`  | **Muestra el detalle completo** de la ejecuci√≥n, incluyendo cada prueba individual que pasa o falla. |
| `docker exec -it disagro_hr_api npm run test -- --coverage` | Genera un informe detallado de la **cobertura** del c√≥digo por pruebas. |

### 2. Pruebas del ETL (Python / Pytest)

Estas son **Pruebas Unitarias** centradas en la l√≥gica de transformaci√≥n (`T`) del ETL, asegurando que las funciones de manipulaci√≥n de datos (`pandas`) y c√°lculo de m√©tricas (como el `trend_score`) sean correctas.

| **Comando**                                                  | **Descripci√≥n**                                              |
| ------------------------------------------------------------ | ------------------------------------------------------------ |
| `docker compose run --rm -v "$(pwd)/tests:/app/tests" etl pytest tests/etl/` | Ejecuta todas las pruebas del m√≥dulo ETL de Python.          |
| `docker compose run --rm -v "$(pwd)/tests:/app/tests" etl pytest -v tests/etl/` | **Muestra el detalle completo** (`-v` de verbose) de la ejecuci√≥n de Pytest, listando el nombre de cada prueba. |





---



## üí° Decisiones de Dise√±o y Justificaciones Clave

1.  **Inclusi√≥n de `language` en `dim_page` y `fact_pageviews_daily`:**
    *   El requisito de la prueba indica que `dim_page` debe incluir `idioma`, y que se deben implementar √≠ndices para acelerar consultas por `lang`. Para cumplir estrictamente con el primer punto, `language` se incluye en `dim_page`.
    *   Simult√°neamente, para optimizar el rendimiento de las consultas anal√≠ticas sobre los datos de pageviews (en `fact_pageviews_daily`) y permitir una indexaci√≥n eficiente por `language` sin costosos `JOIN`s a la tabla de dimensi√≥n en cada consulta (especialmente para los endpoints de la API), se ha decidido **desnormalizar** la columna `language` incluy√©ndola tambi√©n directamente en `fact_pageviews_daily`. 
2.  **Umbral del `trend_score` para T√≠tulos en Tendencia (`>= 2.0`):**
    *   Para la funcionalidad de identificar t√≠tulos en tendencia, se ha establecido un umbral de `trend_score` de `2.0`. Este valor fue elegido para capturar solo los aumentos en vistas que son **claramente fuera de lo com√∫n y verdaderamente significativos** para una p√°gina en particular. El objetivo es que la funcionalidad de tendencia resalte aquellos temas de inter√©s alto y repentino, brindando a los usuarios una visi√≥n m√°s limpia y relevante de lo que realmente est√° destacando en Wikipedia, minimizando el "ruido".
3.  **Uso de `RANK()` en `mv_top_n_daily_by_language`:**
    *   Se utiliz√≥ la funci√≥n de ventana `RANK()` para calcular el ranking diario por idioma. `RANK()` es la funci√≥n apropiada para clasificaciones donde los elementos empatados (p√°ginas con el mismo n√∫mero de vistas) deben compartir la misma posici√≥n.
4.  **Tabla `etl_jobs`:**
    *   Aunque no es un requisito expl√≠cito de la prueba, se ha incluido la tabla `etl_jobs`. Esta tabla sirve para **registrar y rastrear el estado de cada ejecuci√≥n del proceso ETL**, permitiendo que el frontend orqueste y monitoree estas tareas as√≠ncronas en el backend.
5.  **Justificaci√≥n de la Estrategia de Extracci√≥n (BigQuery):**
    - Se opt√≥ por utilizar una √∫nica query con el filtro de rango FORMAT_TIMESTAMP(...) BETWEEN start_date AND end_date para cubrir la ventana de extracci√≥n en lugar de un bucle de consulta diario.
    - Raz√≥n de la Decisi√≥n (Optimizaci√≥n): BigQuery es un motor optimizado para grandes scans. Ejecutar una sola consulta, incluso si abarca m√∫ltiples d√≠as, es significativamente m√°s eficiente que ejecutar un bucle en Python que realiza una consulta por d√≠a. Esto minimiza el overhead de la latencia de red y la inicializaci√≥n de jobs, reduciendo el tiempo total de ejecuci√≥n (TTD) de la fase de Extracci√≥n (E).
6.  **Justificaci√≥n de la Funci√≥n de Filtrado de Fechas:**
    - Se eligi√≥ la funci√≥n FORMAT_TIMESTAMP de BigQuery en lugar de funciones de manipulaci√≥n de strings (SUBSTR) para aplicar los filtros de fecha.
    - Raz√≥n de la Decisi√≥n (Robustez y Partici√≥n): Con esto se asegura que el filtro:
      - Maneje correctamente el tipo de dato TIMESTAMP de la columna datehour.
7.  **Justificaci√≥n de la Fase de Transformaci√≥n (T):**
    - **Agregaciones Anal√≠ticas por Plataforma**
      - Todos los c√°lculos anal√≠ticos (Promedios M√≥viles, Variaciones y Z-Score) se realizaron mediante la funci√≥n df.groupby(['language', 'title', 'platform_type']) en Pandas.
      - Raz√≥n de la Decisi√≥n: La extracci√≥n segmenta las vistas por tipo de plataforma (mobile o desktop). Para tener m√©tricas derivadas precisas y granulares (como la tendencia de la versi√≥n m√≥vil vs. la de escritorio).
    - **M√©trica "Variaciones" (Crecimiento Porcentual Diario)**
      - Para cumplir con el requisito de "variaciones", se implement√≥ la m√©trica Crecimiento Porcentual Diario (variations).
      - Raz√≥n de la Decisi√≥n: Esta m√©trica es fundamental en la anal√≠tica de negocio porque ofrece una se√±al inmediata de volatilidad a corto plazo, comparando las vistas de hoy contra el d√≠a anterior.
    - **Normalizaci√≥n de T√≠tulos (Diacr√≠ticos)**
      - Se a√±adi√≥ un paso de limpieza que utiliza el m√≥dulo unicodedata para la columna title, convirti√©ndola en normalized_title (sin acentos, en min√∫sculas y con espacios reemplazados por guiones bajos).
      - Raz√≥n de la Decisi√≥n (Integridad de la Clave): Esto es crucial para la extracci√≥n del idioma espa√±ol (es) y la construcci√≥n de la dimensi√≥n dim_page. Una normalizaci√≥n incompleta (que deja acentos) provocar√≠a que el mismo concepto (p. ej., "P√°gina de Prueba" vs. "Pagina de Prueba") genere dos claves dimensionales (page_id) diferentes, lo cual es un fallo de integridad en el modelo estrella.
8.  **Justificaci√≥n de ventana de 7 d√≠as:** 
    - Debido a las restricciones de memoria del entorno de ejecuci√≥n local, la prueba de la fase de Transformaci√≥n (T) se ejecut√≥ con una ventana de 7 d√≠as de datos hist√≥ricos. Esta ventana es suficiente para demostrar la funcionalidad de las m√©tricas de avg_views_7d y el correcto funcionamiento del groupby en los c√°lculos rolling y el Z-Score. Para cargas de producci√≥n completas (30+ d√≠as), se requerir√≠a un runtime con mayor asignaci√≥n de RAM



---

**üìö Documentaci√≥n Adicional**

\*   ***Colecci√≥n de Postman:*** 

‚Äã	Se ha proporcionado una colecci√≥n de postman con ejemplos de solicitudes para los endpoints de la API, facilitando su exploraci√≥n y prueba en  `api/docs`.



---

## üßë‚Äçüíª Autor

*   **Hector Ram√≠rez**
*   heckramirezc@aol.com
*   https://www.linkedin.com/in/heckramirez/
*   http://github.com/heckramirezc

---
